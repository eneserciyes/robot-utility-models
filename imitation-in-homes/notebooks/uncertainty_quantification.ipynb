{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hydra\n",
    "from hydra import compose, initialize\n",
    "import matplotlib.pyplot as plt\n",
    "import collections\n",
    "\n",
    "import json\n",
    "import einops\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms.functional as TF\n",
    "\n",
    "\n",
    "from run import _init_model_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with initialize(config_path=\"../configs/\"):\n",
    "    cfg = compose(config_name=\"run_vqbet\", overrides=[\"task=tissue_pick_up\", \"run_offline=True\", \"dataset.test.shuffle_mode=SEQUENTIAL\", \"device=cuda\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = _init_model_loss(cfg)\n",
    "model.eval();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = hydra.utils.instantiate(cfg.dataset.test)\n",
    "test_dataset.set_include_trajectory_end(True)\n",
    "buffer_size = cfg[\"image_buffer_size\"]\n",
    "device = cfg[\"device\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "(input_images, terminate), *_, gt_actions = test_dataset[i]\n",
    "TF.to_pil_image(torchvision.utils.make_grid(input_images))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_model(model, test_dataset, num_steps, repeat_num=1):\n",
    "    action_preds = []\n",
    "    ground_truth = []\n",
    "    images = []\n",
    "    image_buffer = collections.deque(maxlen=buffer_size)\n",
    "    for i in range(num_steps):\n",
    "        # get step from dataset\n",
    "        (input_images, terminate), *_, gt_actions = test_dataset[i]\n",
    "\n",
    "        # prepare input for forward pass\n",
    "        input_images = input_images.float() / 255.0\n",
    "        image_buffer.append(input_images[-1])\n",
    "        img = input_images[-1]\n",
    "        images.append(einops.rearrange(img, \"c h w -> h w c\").cpu().detach().numpy())\n",
    "        ground_truth.append(gt_actions[-1])\n",
    "\n",
    "        model_input = (\n",
    "            torch.stack(tuple(image_buffer), dim=0).unsqueeze(0).repeat(repeat_num, 1, 1, 1, 1).to(device),\n",
    "            torch.tensor(gt_actions).unsqueeze(0).repeat(repeat_num, 1, 1).to(device),\n",
    "        )\n",
    "\n",
    "        # forward pass\n",
    "        out, _ = model.step(model_input)\n",
    "        action_preds.append(out.squeeze().cpu().detach().numpy())\n",
    "\n",
    "    return np.array(action_preds), np.array(ground_truth), np.array(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "action_preds, ground_truth, images = run_model(model, test_dataset, 100, repeat_num=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(action_preds - ground_truth)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Uncertainty quantification\n",
    "\n",
    "- Set model to eval() mode but enable dropout\n",
    "- Do forward pass on the model 64 times.\n",
    "- Look at the standard deviation of the outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "for module in model.modules():\n",
    "    # if model is Dropout, set it to train mode\n",
    "    if isinstance(module, torch.nn.Dropout):\n",
    "        module.train(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "action_preds, ground_truth, images = run_model(model, test_dataset, 100, repeat_num=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "action_pred_xyz_norm = np.linalg.norm(action_preds[:, :, :3], axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "action_pred_xyz_norm.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(action_pred_xyz_norm.std(axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "action_pred_xyz_norm_std= action_pred_xyz_norm.std(axis=1)\n",
    "normalized_action_pred_xyz_std = (action_pred_xyz_norm_std - action_pred_xyz_norm_std.min()) / (action_pred_xyz_norm_std.max() - action_pred_xyz_norm_std.min())\n",
    "plt.plot(normalized_action_pred_xyz_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# put a circle with radius of 0.1 in images[0]\n",
    "import cv2\n",
    "\n",
    "circled_img = cv2.circle(images[0].copy(), center=(128, 200), radius=50, color=(255, 0, 0), thickness=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_frames = []\n",
    "for i, img in enumerate(images):\n",
    "    frame = (img.copy() * 255).astype(np.uint8)\n",
    "    video_frames.append(cv2.circle(frame, center=(128, 200), radius=int(50 * normalized_action_pred_xyz_std[i]), color=(0, 0, 255), thickness=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write video frames to video\n",
    "fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "out = cv2.VideoWriter('uncertainty_quantification.mp4', fourcc, 1.0, (256, 256))\n",
    "\n",
    "for frame in video_frames:\n",
    "    out.write(frame[..., ::-1])\n",
    "\n",
    "out.release()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "home_robot3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
